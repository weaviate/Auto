{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hf_dataset': 'weaviate/WithoutRetrieval-SchemaSplit-Test-80',\n",
       " 'prompt_template': '## Instruction\\nYour task is to write GraphQL for the Natural Language Query provided. Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate.\\n\\nOnly use the API reference to understand the syntax of the request.\\n\\n## Natural Language Query\\n{nlcommand}\\n\\n## Schema\\n{schema}\\n\\n## API reference\\n{apiRef}\\n\\n## Answer\\n```graphql\\n',\n",
       " 'push_to_hub': 'substratusai/wgql-WithRetrieval-SchemaSplit-Train-80'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "params = {}\n",
    "params_path = Path(\"/content/params.json\")\n",
    "if params_path.is_file():\n",
    "    with params_path.open(\"r\", encoding=\"UTF-8\") as params_file:\n",
    "        params = json.load(params_file)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acaacf2745249ef8263941e4aff19cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44395c0c633a4af5beef6d63326b0787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab4116fcdf84717961ad3e401fc3432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.90M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c699b24e1c824ab88502979d97fd3133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0314a375e94a1d96e59e4d894dd9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'nlcommand', 'apiRef', 'apiRefPath', 'schema', 'schemaPath'],\n",
       "        num_rows: 825\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "hf_dataset = params.get(\"hf_dataset\")\n",
    "if hf_dataset:\n",
    "    dataset = load_dataset(hf_dataset)\n",
    "else:\n",
    "    dataset = load_dataset(\"json\", data_files=\"/content/data/*.json*\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Instruction\n",
      "Your task is to write GraphQL for the Natural Language Query provided. Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate.\n",
      "\n",
      "Only use the API reference to understand the syntax of the request.\n",
      "\n",
      "## Natural Language Query\n",
      "```text\n",
      "Show me the event name, description, year, significant impact, and the countries involved with their population for the top 10 historical events.\n",
      "```\n",
      "\n",
      "## Schema\n",
      "{\n",
      "\"classes\": [\n",
      "{\n",
      "\"class\": \"HistoricalEvent\",\n",
      "\"description\": \"Information about historical events\",\n",
      "\"vectorIndexType\": \"hnsw\",\n",
      "\"vectorizer\": \"text2vec-transformers\",\n",
      "\"properties\": [\n",
      "{\n",
      "\"name\": \"eventName\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Name of the historical event\"\n",
      "},\n",
      "{\n",
      "\"name\": \"description\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Detailed description of the event\"\n",
      "},\n",
      "{\n",
      "\"name\": \"year\",\n",
      "\"dataType\": [\"int\"],\n",
      "\"description\": \"Year the event occurred\"\n",
      "},\n",
      "{\n",
      "\"name\": \"hadSignificantImpact\",\n",
      "\"dataType\": [\"boolean\"],\n",
      "\"description\": \"Whether the event had a significant impact\"\n",
      "},\n",
      "{\n",
      "\"name\": \"involvedCountries\",\n",
      "\"dataType\": [\"Country\"],\n",
      "\"description\": \"Countries involved in the event\"\n",
      "}{\n",
      "\"class\": \"Country\",\n",
      "\"description\": \"Information about countries\",\n",
      "\"vectorIndexType\": \"hnsw\",\n",
      "\"vectorizer\": \"text2vec-transformers\",\n",
      "\"properties\": [\n",
      "{\n",
      "\"name\": \"countryName\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Name of the country\"\n",
      "},\n",
      "{\n",
      "\"name\": \"population\",\n",
      "\"dataType\": [\"int\"],\n",
      "\"description\": \"Population of the country\"\n",
      "}}}\n",
      "\n",
      "## API reference\n",
      "`limit` returned objects\n",
      "\n",
      "Often, you will only want the top `n` results from the query. This can be achieved by setting a `limit` as shown below.\n",
      "\n",
      "```graphql\n",
      "{\n",
      "  Get {\n",
      "    JeopardyQuestion (\n",
      "      limit: 1\n",
      "    ) {\n",
      "      question\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## Answer\n",
      "```graphql\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_prompt = \"\"\"\n",
    "## Instruction\n",
    "Your task is to write GraphQL for the Natural Language Query provided. Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate.\n",
    "\n",
    "Only use the API reference to understand the syntax of the request.\n",
    "\n",
    "## Natural Language Query\n",
    "{nlcommand}\n",
    "\n",
    "## Schema\n",
    "{schema}\n",
    "\n",
    "## API reference\n",
    "{apiRef}\n",
    "\n",
    "## Answer\n",
    "```graphql\n",
    "\"\"\"\n",
    "\n",
    "prompt = params.get(\"prompt_template\", default_prompt)\n",
    "print(prompt.format_map(dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d48ffca0eb44a248a23655f594f84e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"/content/model/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, device_map=\"auto\", trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16, \n",
    "            use_flash_attention_2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 18 02:24:32 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA L4           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   52C    P0    29W /  72W |  13676MiB / 23034MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"max_length\": 4096,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"temperature\": 0.6,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7521])\n",
      "False\n",
      "False\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "stop_ids = torch.LongTensor(tokenizer.encode(\"```\", add_special_tokens=False))\n",
    "## Note the stop_ids aren't correct, for some reason there are multiple possible token IDs for ```\n",
    "## so instead we're using tensor([13940, 28832], device='cuda:0') as the stop_ids, because that's\n",
    "## what the model normally generates\n",
    "print(stop_ids)\n",
    "print(tokenizer.decode([8789]) == \"```\")\n",
    "print(tokenizer.decode([13940, 28832]) == \"```\")\n",
    "print(tokenizer.decode(tokenizer.encode(\"```\", add_special_tokens=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class BacktickStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        if self.tokenizer.decode(input_ids[0][-2:]) == \"```\" or self.tokenizer.decode(input_ids[0][-1]) == \"```\":\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([BacktickStoppingCriteria(tokenizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.bos_token_id = tokenizer.bos_token_id = 1\n",
    "model.config.eos_token_id = tokenizer.eos_token_id = 2\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Instruction\n",
      "Your task is to write GraphQL for the Natural Language Query provided. Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate.\n",
      "\n",
      "Only use the API reference to understand the syntax of the request.\n",
      "\n",
      "## Natural Language Query\n",
      "```text\n",
      "Show me the event name, description, year, significant impact, and the countries involved with their population for the top 10 historical events.\n",
      "```\n",
      "\n",
      "## Schema\n",
      "{\n",
      "\"classes\": [\n",
      "{\n",
      "\"class\": \"HistoricalEvent\",\n",
      "\"description\": \"Information about historical events\",\n",
      "\"vectorIndexType\": \"hnsw\",\n",
      "\"vectorizer\": \"text2vec-transformers\",\n",
      "\"properties\": [\n",
      "{\n",
      "\"name\": \"eventName\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Name of the historical event\"\n",
      "},\n",
      "{\n",
      "\"name\": \"description\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Detailed description of the event\"\n",
      "},\n",
      "{\n",
      "\"name\": \"year\",\n",
      "\"dataType\": [\"int\"],\n",
      "\"description\": \"Year the event occurred\"\n",
      "},\n",
      "{\n",
      "\"name\": \"hadSignificantImpact\",\n",
      "\"dataType\": [\"boolean\"],\n",
      "\"description\": \"Whether the event had a significant impact\"\n",
      "},\n",
      "{\n",
      "\"name\": \"involvedCountries\",\n",
      "\"dataType\": [\"Country\"],\n",
      "\"description\": \"Countries involved in the event\"\n",
      "}{\n",
      "\"class\": \"Country\",\n",
      "\"description\": \"Information about countries\",\n",
      "\"vectorIndexType\": \"hnsw\",\n",
      "\"vectorizer\": \"text2vec-transformers\",\n",
      "\"properties\": [\n",
      "{\n",
      "\"name\": \"countryName\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Name of the country\"\n",
      "},\n",
      "{\n",
      "\"name\": \"population\",\n",
      "\"dataType\": [\"int\"],\n",
      "\"description\": \"Population of the country\"\n",
      "}}}\n",
      "\n",
      "## API reference\n",
      "`limit` returned objects\n",
      "\n",
      "Often, you will only want the top `n` results from the query. This can be achieved by setting a `limit` as shown below.\n",
      "\n",
      "```graphql\n",
      "{\n",
      "  Get {\n",
      "    JeopardyQuestion (\n",
      "      limit: 1\n",
      "    ) {\n",
      "      question\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## Answer\n",
      "```graphql\n",
      " Aut Autaut AutAut autAut AutAutAutJe AutautJePhoneAutAutčka Aut Aut AutJeAut aut AutAutAut Aut AutAut AutAut AutJečkaAututt Aut Aut PonJe AutAut Aut aut Aut autAutJeAutAutPhone Aut JeautJeautPhone autJeJeJe AutAutAut Autčka AutAut Aut Autaut Aut AutAut AutAutPhone AutAut AutpngAut安Autčkaaut AutPhone Aututt AutčkaJeAut Aut Aut AutAut Aut aut  AutAutAutčka AutOIN AutPhone Aut Aut Aut Aut AutAut AutPhonePhone AutAutPhoneAut autAut Autpng Aut AutAut Aut Aut Aut Aut AutAutAut autJe AutAut aut AutAutAutAut aut Aut Aut Aut AutPhoneaut Autaut AutAutaut AutPhone Aut Aut AutAut AutAut aut AutAutčka Aut AutAut AutAut autńskiAutpng Autwick AutAutńskiAut autčkaAut AutPhoneJeAutAutčkaAut AutJeAut Aut aut AutAutPhoneAutAut AutAutAGAutAut Autaut aut AutautAut aut Aut Aut Aut AutAutAut Aut Autaut autpng AutPhoneAutAut aut AutPhone Aut Aut Autński AutJeautčkaAutPhone AutPhoneAutAut aut autAut Aut aut AutAutAut Aut AutAut AutčkaAutčkaAutAutčkaJe Aut AutPhone Aut AutPhone AutAut AutAut AutPhone aut AutAut AutAut AutAGAutAutčkaAut AutAut autAut Aut Aut Aut Aut\n",
      "CPU times: user 19 s, sys: 0 ns, total: 19 s\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "model_inputs = tokenizer([prompt.format_map(dataset[\"train\"][0])],\n",
    "                         return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs,\n",
    "                               max_new_tokens=300,\n",
    "                               stopping_criteria=stopping_criteria)\n",
    "\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 534])\n",
      "aut aut Aut Aut AutautAutJeAutčka aut autJePhoneautAutAut Aut AutAututt AutautJeommAut AutAut Autaut AutAutЉAutPhonewickAut  Aut AututtJeJeAut Autčka Autński AutAut autPhone autPhonečka Aut AutAut autAut Aut Aut Aututt AutJe Aut Aut Autaut AutAut aut Aut AutAutAutAutAutPhoneAutAut AutAutčkaAut Aut aut aut Aut autčkaAutJePhone Aut Autaut aut Aut aut AutAut AutAutJe Aut Aut AutAut autAut autAut Aut  AutAutPhone Aut Aut Aut Aut autJePhone aut Aut AutJeJePhoneAutAG Aut AutAutAut aut AutAut AutAutAut AutAut AutAutAutpngAut AutPhoneAut Aut aut AutAut Aut autAut AutAutAut Aut AutAut AutJe Aut autAutAutJe Aut AutPhoneAutAutAutAutautčka autautAut Aut Aut AutAutPhone Aut AutautAut AutAG AutAut AutAut Aut AutAut autAutAutAutaut Aut Aut Aut Aut AutAutAut Aut Aut autAut Aut AutAutAut AutAutAut Aut Aut AutAutAutAut Aut Aut Aut Aut AutAutAut Autaut Aut Aut Autaut autAutAut AutAut Autčka AutAut Aut Aut AutPhoneAut Aut aut Aut Aut Aut Aut autAut Aut Aut autAutAut Aut Aut AutautAutPhoneAutAG Aut AutAut AutAutJeAutPhoneJePhoneAG AutJeAut Aut Aut Aut Aut\n"
     ]
    }
   ],
   "source": [
    "print(model_inputs[\"input_ids\"].shape)\n",
    "input_length = model_inputs[\"input_ids\"].shape[1]\n",
    "print(tokenizer.decode(generated_ids[0][input_length:], skip_special_tokens=True).strip(\"```\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "dataset_size = len(dataset[\"train\"])\n",
    "output_path = \"/content/artifacts/test-output.json\"\n",
    "entries = []\n",
    "print(f\"Running inference for {dataset_size} entries in dataset\")\n",
    "for i in range(dataset_size):\n",
    "    print(f\"entry {i+1} of {dataset_size}\")\n",
    "    entry = dataset[\"train\"][i]\n",
    "    model_inputs = tokenizer([prompt.format_map(entry)],\n",
    "                         return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "\n",
    "    generated_ids = model.generate(**model_inputs,\n",
    "                               max_new_tokens=300,\n",
    "                               stopping_criteria=stopping_criteria)\n",
    "    input_length = model_inputs[\"input_ids\"].shape[1]\n",
    "    output = tokenizer.decode(generated_ids[0][input_length:], skip_special_tokens=True)\n",
    "    entry[\"modelOutput\"] = output.strip(\"```\")\n",
    "    entries.append(entry)\n",
    "\n",
    "    with open(output_path, 'a') as file:\n",
    "        json.dump(entry, file)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the test dataset with model output in the original HuggingFace Model repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "repo_id = params.get(\"push_to_hub\")\n",
    "if repo_id:\n",
    "    hf_api = HfApi()\n",
    "    hf_api.upload_file(\n",
    "            path_or_fileobj=Path(output_path),\n",
    "            path_in_repo=Path(output_path).name,\n",
    "            repo_id=repo_id,\n",
    "    )\n",
    "    logs_path = Path(\"/content/artifacts/eval.ipynb\")\n",
    "    if logs_path.exists():\n",
    "        hf_api.upload_file(\n",
    "            path_or_fileobj=logs_path,\n",
    "            path_in_repo=logs_path.name,\n",
    "            repo_id=repo_id,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the model output on a live Weaviate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: []\n"
     ]
    }
   ],
   "source": [
    "valid = []\n",
    "for i in range(len(dataset[\"train\"])):\n",
    "    try:\n",
    "        json.loads(dataset[\"train\"][i][\"schema\"])\n",
    "        valid.append(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Valid:\", valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import weaviate\n",
    "# from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "# client = weaviate.Client(\n",
    "#   embedded_options=EmbeddedOptions()\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
