{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hf_dataset': 'weaviate/WithoutRetrieval-SchemaSplit-Test-80',\n",
       " 'push_to_hub': 'substratusai/wgql-WithRetrieval-SchemaSplit-Train-80-3epochs'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "params = {}\n",
    "params_path = Path(\"/content/params.json\")\n",
    "if params_path.is_file():\n",
    "    with params_path.open(\"r\", encoding=\"UTF-8\") as params_file:\n",
    "        params = json.load(params_file)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b40d12882d4f8c95d7863d9efbe4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e195eb7b9e4c2ca204b3d00440f407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535df09e22df4005a585b50b98fa3f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.90M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f6a6a29072498d9af2032b3ebaeccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725106d440954b43a998872e3f1d73e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'nlcommand', 'apiRef', 'apiRefPath', 'schema', 'schemaPath'],\n",
       "        num_rows: 825\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "hf_dataset = params.get(\"hf_dataset\")\n",
    "if hf_dataset:\n",
    "    dataset = load_dataset(hf_dataset)\n",
    "else:\n",
    "    dataset = load_dataset(\"json\", data_files=\"/content/data/*.json*\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Instruction\n",
      "Your task is to write GraphQL for the Natural Language Query provided. Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate.\n",
      "\n",
      "Only use the API reference to understand the syntax of the request.\n",
      "\n",
      "## Natural Language Query\n",
      "```text\n",
      "Show me the event name, description, year, significant impact, and the countries involved with their population for the top 10 historical events.\n",
      "```\n",
      "\n",
      "## Schema\n",
      "{\n",
      "\"classes\": [\n",
      "{\n",
      "\"class\": \"HistoricalEvent\",\n",
      "\"description\": \"Information about historical events\",\n",
      "\"vectorIndexType\": \"hnsw\",\n",
      "\"vectorizer\": \"text2vec-transformers\",\n",
      "\"properties\": [\n",
      "{\n",
      "\"name\": \"eventName\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Name of the historical event\"\n",
      "},\n",
      "{\n",
      "\"name\": \"description\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Detailed description of the event\"\n",
      "},\n",
      "{\n",
      "\"name\": \"year\",\n",
      "\"dataType\": [\"int\"],\n",
      "\"description\": \"Year the event occurred\"\n",
      "},\n",
      "{\n",
      "\"name\": \"hadSignificantImpact\",\n",
      "\"dataType\": [\"boolean\"],\n",
      "\"description\": \"Whether the event had a significant impact\"\n",
      "},\n",
      "{\n",
      "\"name\": \"involvedCountries\",\n",
      "\"dataType\": [\"Country\"],\n",
      "\"description\": \"Countries involved in the event\"\n",
      "}{\n",
      "\"class\": \"Country\",\n",
      "\"description\": \"Information about countries\",\n",
      "\"vectorIndexType\": \"hnsw\",\n",
      "\"vectorizer\": \"text2vec-transformers\",\n",
      "\"properties\": [\n",
      "{\n",
      "\"name\": \"countryName\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Name of the country\"\n",
      "},\n",
      "{\n",
      "\"name\": \"population\",\n",
      "\"dataType\": [\"int\"],\n",
      "\"description\": \"Population of the country\"\n",
      "}}}\n",
      "\n",
      "## API reference\n",
      "`limit` returned objects\n",
      "\n",
      "Often, you will only want the top `n` results from the query. This can be achieved by setting a `limit` as shown below.\n",
      "\n",
      "```graphql\n",
      "{\n",
      "  Get {\n",
      "    JeopardyQuestion (\n",
      "      limit: 1\n",
      "    ) {\n",
      "      question\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## Answer\n",
      "```graphql\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_prompt = \"\"\"\n",
    "## Instruction\n",
    "Your task is to write GraphQL for the Natural Language Query provided. Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate.\n",
    "\n",
    "Only use the API reference to understand the syntax of the request.\n",
    "\n",
    "## Natural Language Query\n",
    "{nlcommand}\n",
    "\n",
    "## Schema\n",
    "{schema}\n",
    "\n",
    "## API reference\n",
    "{apiRef}\n",
    "\n",
    "## Answer\n",
    "```graphql\n",
    "\"\"\"\n",
    "\n",
    "prompt = params.get(\"prompt_template\", default_prompt)\n",
    "print(prompt.format_map(dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc97ef6c2974ab98b172f17b9251731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"/content/model/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, device_map=\"auto\", trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16, \n",
    "            use_flash_attention_2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 15 07:03:54 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA L4           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   48C    P0    29W /  72W |  15452MiB / 23034MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8789])\n",
      "True\n",
      "True\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "stop_ids = torch.LongTensor(tokenizer.encode(\"```\", add_special_tokens=False))\n",
    "## Note the stop_ids aren't correct, for some reason there are multiple possible token IDs for ```\n",
    "## so instead we're using tensor([13940, 28832], device='cuda:0') as the stop_ids, because that's\n",
    "## what the model normally generates\n",
    "print(stop_ids)\n",
    "print(tokenizer.decode([8789]) == \"```\")\n",
    "print(tokenizer.decode([13940, 28832]) == \"```\")\n",
    "print(tokenizer.decode(tokenizer.encode(\"```\", add_special_tokens=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class BacktickStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        if self.tokenizer.decode(input_ids[0][-2:]) == \"```\" or self.tokenizer.decode(input_ids[0][-1]) == \"```\":\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([BacktickStoppingCriteria(tokenizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Instruction\n",
      "Your task is to write GraphQL for the Natural Language Query provided. Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate.\n",
      "\n",
      "Only use the API reference to understand the syntax of the request.\n",
      "\n",
      "## Natural Language Query\n",
      "```text\n",
      "Show me the event name, description, year, significant impact, and the countries involved with their population for the top 10 historical events.\n",
      "```\n",
      "\n",
      "## Schema\n",
      "{\n",
      "\"classes\": [\n",
      "{\n",
      "\"class\": \"HistoricalEvent\",\n",
      "\"description\": \"Information about historical events\",\n",
      "\"vectorIndexType\": \"hnsw\",\n",
      "\"vectorizer\": \"text2vec-transformers\",\n",
      "\"properties\": [\n",
      "{\n",
      "\"name\": \"eventName\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Name of the historical event\"\n",
      "},\n",
      "{\n",
      "\"name\": \"description\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Detailed description of the event\"\n",
      "},\n",
      "{\n",
      "\"name\": \"year\",\n",
      "\"dataType\": [\"int\"],\n",
      "\"description\": \"Year the event occurred\"\n",
      "},\n",
      "{\n",
      "\"name\": \"hadSignificantImpact\",\n",
      "\"dataType\": [\"boolean\"],\n",
      "\"description\": \"Whether the event had a significant impact\"\n",
      "},\n",
      "{\n",
      "\"name\": \"involvedCountries\",\n",
      "\"dataType\": [\"Country\"],\n",
      "\"description\": \"Countries involved in the event\"\n",
      "}{\n",
      "\"class\": \"Country\",\n",
      "\"description\": \"Information about countries\",\n",
      "\"vectorIndexType\": \"hnsw\",\n",
      "\"vectorizer\": \"text2vec-transformers\",\n",
      "\"properties\": [\n",
      "{\n",
      "\"name\": \"countryName\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Name of the country\"\n",
      "},\n",
      "{\n",
      "\"name\": \"population\",\n",
      "\"dataType\": [\"int\"],\n",
      "\"description\": \"Population of the country\"\n",
      "}}}\n",
      "\n",
      "## API reference\n",
      "`limit` returned objects\n",
      "\n",
      "Often, you will only want the top `n` results from the query. This can be achieved by setting a `limit` as shown below.\n",
      "\n",
      "```graphql\n",
      "{\n",
      "  Get {\n",
      "    JeopardyQuestion (\n",
      "      limit: 1\n",
      "    ) {\n",
      "      question\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## Answer\n",
      "```graphql\n",
      "{\n",
      "  Get {\n",
      "    HistoricalEvent (\n",
      "      limit: 10\n",
      "    ) {\n",
      "      eventName\n",
      "      description\n",
      "      year\n",
      "      hadSignificantImpact\n",
      "      involvedCountries {\n",
      "        ... on Country {\n",
      "          countryName\n",
      "          population\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "CPU times: user 17.7 s, sys: 0 ns, total: 17.7 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "model_inputs = tokenizer([prompt.format_map(dataset[\"train\"][0])],\n",
    "                         return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs,\n",
    "                               max_new_tokens=300,\n",
    "                               stopping_criteria=stopping_criteria)\n",
    "\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 537])\n",
      "{\n",
      "  Get {\n",
      "    HistoricalEvent (\n",
      "      limit: 10\n",
      "    ) {\n",
      "      eventName\n",
      "      description\n",
      "      year\n",
      "      hadSignificantImpact\n",
      "      involvedCountries {\n",
      "        ... on Country {\n",
      "          countryName\n",
      "          population\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_inputs[\"input_ids\"].shape)\n",
    "input_length = model_inputs[\"input_ids\"].shape[1]\n",
    "print(tokenizer.decode(generated_ids[0][input_length:], skip_special_tokens=True).strip(\"```\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "dataset_size = len(dataset[\"train\"])\n",
    "output_path = \"/content/artifacts/test-output.json\"\n",
    "entries = []\n",
    "print(f\"Running inference for {dataset_size} entries in dataset\")\n",
    "for i in range(dataset_size):\n",
    "    print(f\"entry {i+1} of {dataset_size}\")\n",
    "    entry = dataset[\"train\"][i]\n",
    "    model_inputs = tokenizer([prompt.format_map(entry)],\n",
    "                         return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "\n",
    "    generated_ids = model.generate(**model_inputs,\n",
    "                               max_new_tokens=300,\n",
    "                               stopping_criteria=stopping_criteria)\n",
    "    input_length = model_inputs[\"input_ids\"].shape[1]\n",
    "    output = tokenizer.decode(generated_ids[0][input_length:], skip_special_tokens=True)\n",
    "    entry[\"modelOutput\"] = output.strip(\"```\")\n",
    "\n",
    "    with open(output_path, 'a') as file:\n",
    "        json.dump(entry, file)\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = params.get(\"push_to_hub\")\n",
    "if repo_id:\n",
    "    hf_api.upload_file(\n",
    "            path_or_fileobj=Path(output_path),\n",
    "            path_in_repo=Path(output_path).name,\n",
    "            repo_id=repo_id,\n",
    "    )\n",
    "    logs_path = Path(\"/content/artifacts/eval.ipynb\")\n",
    "    if logs_path.exists():\n",
    "        hf_api.upload_file(\n",
    "            path_or_fileobj=logs_path,\n",
    "            path_in_repo=logs_path.name,\n",
    "            repo_id=repo_id,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
