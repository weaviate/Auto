{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hf_dataset': 'weaviate/WithoutRetrieval-SchemaSplit-Test-80',\n",
       " 'push_to_hub': 'substratusai/wgql-WithRetrieval-SchemaSplit-Train-80-3epochs'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "params = {}\n",
    "params_path = Path(\"/content/params.json\")\n",
    "if params_path.is_file():\n",
    "    with params_path.open(\"r\", encoding=\"UTF-8\") as params_file:\n",
    "        params = json.load(params_file)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49379f5081bd4c488f125d4949268c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ebc38a6e92420d9e5805f8584dc613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e320fd3f29ee43c79de9258df8147c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.90M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fbc5bb932a450ba38ac102491a872f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f47c83f3b1f4c789e3d3e2e3d95466d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'nlcommand', 'apiRef', 'apiRefPath', 'schema', 'schemaPath'],\n",
       "        num_rows: 825\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "hf_dataset = params.get(\"hf_dataset\")\n",
    "if hf_dataset:\n",
    "    dataset = load_dataset(hf_dataset)\n",
    "else:\n",
    "    dataset = load_dataset(\"json\", data_files=\"/content/data/*.json*\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Instruction\n",
      "Your task is to write GraphQL for the Natural Language Query provided. Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate.\n",
      "\n",
      "Only use the API reference to understand the syntax of the request.\n",
      "\n",
      "## Natural Language Query\n",
      "```text\n",
      "Show me the event name, description, year, significant impact, and the countries involved with their population for the top 10 historical events.\n",
      "```\n",
      "\n",
      "## Schema\n",
      "{\n",
      "\"classes\": [\n",
      "{\n",
      "\"class\": \"HistoricalEvent\",\n",
      "\"description\": \"Information about historical events\",\n",
      "\"vectorIndexType\": \"hnsw\",\n",
      "\"vectorizer\": \"text2vec-transformers\",\n",
      "\"properties\": [\n",
      "{\n",
      "\"name\": \"eventName\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Name of the historical event\"\n",
      "},\n",
      "{\n",
      "\"name\": \"description\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Detailed description of the event\"\n",
      "},\n",
      "{\n",
      "\"name\": \"year\",\n",
      "\"dataType\": [\"int\"],\n",
      "\"description\": \"Year the event occurred\"\n",
      "},\n",
      "{\n",
      "\"name\": \"hadSignificantImpact\",\n",
      "\"dataType\": [\"boolean\"],\n",
      "\"description\": \"Whether the event had a significant impact\"\n",
      "},\n",
      "{\n",
      "\"name\": \"involvedCountries\",\n",
      "\"dataType\": [\"Country\"],\n",
      "\"description\": \"Countries involved in the event\"\n",
      "}{\n",
      "\"class\": \"Country\",\n",
      "\"description\": \"Information about countries\",\n",
      "\"vectorIndexType\": \"hnsw\",\n",
      "\"vectorizer\": \"text2vec-transformers\",\n",
      "\"properties\": [\n",
      "{\n",
      "\"name\": \"countryName\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Name of the country\"\n",
      "},\n",
      "{\n",
      "\"name\": \"population\",\n",
      "\"dataType\": [\"int\"],\n",
      "\"description\": \"Population of the country\"\n",
      "}}}\n",
      "\n",
      "## API reference\n",
      "`limit` returned objects\n",
      "\n",
      "Often, you will only want the top `n` results from the query. This can be achieved by setting a `limit` as shown below.\n",
      "\n",
      "```graphql\n",
      "{\n",
      "  Get {\n",
      "    JeopardyQuestion (\n",
      "      limit: 1\n",
      "    ) {\n",
      "      question\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## Answer\n",
      "```graphql\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_prompt = \"\"\"\n",
    "## Instruction\n",
    "Your task is to write GraphQL for the Natural Language Query provided. Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate.\n",
    "\n",
    "Only use the API reference to understand the syntax of the request.\n",
    "\n",
    "## Natural Language Query\n",
    "{nlcommand}\n",
    "\n",
    "## Schema\n",
    "{schema}\n",
    "\n",
    "## API reference\n",
    "{apiRef}\n",
    "\n",
    "## Answer\n",
    "```graphql\n",
    "\"\"\"\n",
    "\n",
    "prompt = params.get(\"prompt_template\", default_prompt)\n",
    "print(prompt.format_map(dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de20637601e449b08a035b0d39453e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"/content/model/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, load_in_8bit=True, device_map=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 14 06:52:37 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA L4           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   60C    P0    32W /  72W |  19552MiB / 23034MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "## Instruction\n",
      "Your task is to write GraphQL for the Natural Language Query provided. Use the provided API reference and Schema to generate the GraphQL. The GraphQL should be valid for Weaviate.\n",
      "\n",
      "Only use the API reference to understand the syntax of the request.\n",
      "\n",
      "## Natural Language Query\n",
      "```text\n",
      "Show me the name, description, year introduced, and whether it is a string of 10 instruments. Also, show me the name, genre, and years active of the musicians who play those instruments.\n",
      "```\n",
      "\n",
      "## Schema\n",
      "{\n",
      "\"classes\": [\n",
      "{\n",
      "\"class\": \"Instrument\",\n",
      "\"description\": \"A musical instrument.\",\n",
      "\"vectorIndexType\": \"hnsw\",\n",
      "\"vectorizer\": \"text2vec-transformers\",\n",
      "\"properties\": [\n",
      "{\n",
      "\"name\": \"name\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Name of the instrument.\"\n",
      "},\n",
      "{\n",
      "\"name\": \"description\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Description of the instrument.\"\n",
      "},\n",
      "{\n",
      "\"name\": \"yearIntroduced\",\n",
      "\"dataType\": [\"int\"],\n",
      "\"description\": \"Year the instrument was introduced.\"\n",
      "},\n",
      "{\n",
      "\"name\": \"isString\",\n",
      "\"dataType\": [\"boolean\"],\n",
      "\"description\": \"Is it a string instrument?\"\n",
      "},\n",
      "{\n",
      "\"name\": \"playedBy\",\n",
      "\"dataType\": [\"Musician\"],\n",
      "\"description\": \"Musicians who play this instrument.\"\n",
      "}{\n",
      "\"class\": \"Musician\",\n",
      "\"description\": \"An individual who plays a musical instrument.\",\n",
      "\"vectorIndexType\": \"hnsw\",\n",
      "\"vectorizer\": \"text2vec-transformers\",\n",
      "\"properties\": [\n",
      "{\n",
      "\"name\": \"name\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Name of the musician.\"\n",
      "},\n",
      "{\n",
      "\"name\": \"genre\",\n",
      "\"dataType\": [\"text\"],\n",
      "\"description\": \"Genre of music the musician specializes in.\"\n",
      "},\n",
      "{\n",
      "\"name\": \"yearsActive\",\n",
      "\"dataType\": [\"int\"],\n",
      "\"description\": \"Years active in music.\"\n",
      "}}}\n",
      "\n",
      "## API reference\n",
      "`limit` returned objects\n",
      "\n",
      "Often, you will only want the top `n` results from the query. This can be achieved by setting a `limit` as shown below.\n",
      "\n",
      "```graphql\n",
      "{\n",
      "  Get {\n",
      "    JeopardyQuestion (\n",
      "      limit: 1\n",
      "    ) {\n",
      "      question\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## Answer\n",
      "```graphql\n",
      "{\n",
      "{\n",
      "\"class\" : {\n",
      "\"class\" } {\n",
      "\"class\" } {\n",
      "\"class\" } {\n",
      "\"class\" } {\n",
      "\"class\" } {\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "{\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model_inputs = tokenizer([prompt.format_map(dataset[\"train\"][0])], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95, temperature=0.3)\n",
    "print(tokenizer.batch_decode(generated_ids)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
